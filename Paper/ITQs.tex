\section{Information theory quantifiers}\label{sec:quant}

The first step to quantify the statistical properties of the values (amplitude statistics) of a time series $\{x_i,~(i=1,...,N)\}$, using information theory is to determine the concomitant PDF because all the quantifiers are functionals of the PDF associated to the time series. This is an issue studied in detail in previous papers \cite{aka varios}. Let us summarize the procedure:

\begin{enumerate} 
\item \label{1} a finite alphabet with $M$ symbols $\mathcal{A}=\{a_1,...,a_M\}$ is chosen. 
\item \label{2} one of these symbols is assigned: (a) to each value of the time series of (b) to each portion of length $D$ of the trajectory. 
\item \label{3} the normalized histogram of the symbols is the desired $PDF$.
\end{enumerate}

Note that if option (a) is chosen in step \ref{2} then the PDF is \textit{non causal}, because all the information about the time evolution of the system generating $\{x_i\}$ is completely lost. On the contrary if option (b) is chosen in step \ref{2} then the PDF is \textit{causal}, in the sense it has some information about the temporal evolution.

Of course there are infinite possibilities to choose the alphabet $\mathcal{A}$ as well as the length $D$.
Bandt \& Pompe made a proposal for a causal PDF that has been shown to be easy to implement and useful in a great variety of applications.
The procedure is the following \cite{Pompe2002,Keller2003,Keller2005}: \textcolor{red}{ESTA PARTE ESTÁ EN EL TIEMPO Y DEBERÍA ESTAR EN LAS MUESTRAS}
\begin{itemize}
\item Given a series $\{x_t : t=0, \Delta t, \cdots,M\Delta t \}$, a sequence of vectors of length $d$ is generated.

\begin{equation}
\label{eq:vectores}
(s)\mapsto \left(x_{t-(d-1)\Delta t},x_{t-(d-2)\Delta t},\cdots,x_{t-\Delta t},x_{t}\right) \ ,
\end{equation}

Each vector turns out to be the ``history'' of the value $x_t$. Clearly, the longer the length of the vectors $D$, the more information about the history would the vectors have but a higher value of $N$ is required to have an adequate statistics. 

\item The permutations $\pi=(r_0, r_1, \cdots, r_{D-1})$ of $(0, 1, \cdots, D-1)$ are called ``order of patterns'' of time $t$, defined by:

\begin{equation}
\label{eq:permuta}
x_{t-r_{D-1}\Delta t}\le x_{t-r_{D-2}\Delta t}\le\cdots\le x_{t-r_{1}\Delta t}\le x_{t-r_0\Delta t}.
\end{equation}

In order to obtain an unique result it is considered $r_i<r_{i-1}$ if $x_{t-r_{i}\Delta t}=x_{t-r_{i-1}\Delta t}$.

In this way, all the $D!$ possible permutations $\pi$ of order $D$, and the PDF $P=\{p(\pi)\}$ is defined as:

\begin{equation}
\label{eq:frequ}
p(\pi)=\frac{\sharp \{s|s\leq M-D+1; (s) \quad \texttt{has type}~\pi\}}{M-D+1}.
\end{equation}

In the last expression the $\sharp$ symbol means ``number".
\end{itemize}

This procedure has the advantages of being {\it i)} simple, {\it ii)} fast to calculate, {\it iii)} robust in presence of noise, and {\it iv)} invariant to lineal monotonous transformations. \textcolor{red}{DICE QUE ES ROBUSTO FRENTE A LA PRESENCIA DE RUIDO PERO NO ES ASÍ}

It is applicable to weak stationarity processes (for $k=D$, the probability that $x_t < x_{t+k}$ doesn't depend on the particulary $t$ \cite{Pompe2002}).The causality property of the PDF allows the quantifiers (based on this PDFs) to discriminate between deterministic and stochastic systems \cite{Rosso2007B}.

According to this point Bandt and Pompe suggested $3\leq D \leq 7$. $D=6$ has been adopted in this work.

Based on our previous research \cite{DeMicco2008,DeMicco2009} we have employed two $PDF$'s: (a) the normalized histogram of the time series amplitudes $\{x_i\}$ (that is a non-causal $PDF$), and (b) the Bandt \& Pompe $PDF$ (that is a causal $PDF$).
The entropies $H_{hist}$ and $H_{BP}$, the statistical complexity $C_{BP}$ are used as quantifiers. The entropies $H_{hist}$ and $H_{BP}$ are the normalized version of the Of course there are infinite possibilities to choose the alphabet as well as the length $d$.

We also used the number of missing patterns $MP$ as a quantifier\cite{Rosso2012}.
As shown recently by Amig\'o {\it et al.} \cite{Amigo2006,Amigo2007,Amigo2008,Amigo2010}, in the case of deterministic one-dimensional maps, not all the possible ordinal patterns can be effectively materialized into orbits, which in a sense makes these patterns ``forbidden".
Indeed, the existence of these {\it forbidden ordinal patterns} becomes a persistent fact that can be regarded as a ``new" dynamical property.
Thus, for a fixed pattern-length (embedding dimension $D$) the number of forbidden patterns of a time series (unobserved patterns) is independent of the series length $N$.
Remark that this independence does not characterize other properties of the series such as proximity and correlation, which die out with time \cite{Amigo2007,Amigo2010}.

A full discussion about the convenience of using these quantifiers is out of the scope of this work.
Nevertheless reliable bibliographic sources do exist \cite{Wackerbauer1994,Lopez1995,Rosso2007A,DeMicco2008,Rosso2009,Martin2006,Rosso2012}.


Bandt \& Pompe made a proposal for a causal PDF that has been shown to be easy to implement and useful in a great variety of applications. The procedure is the
following \cite{Pompe2002,Keller2003,Keller2005}: a) Given a
series $\{x_t : t=0, \Delta t, \cdots,M\Delta t \}$, a sequence of
vectors of length $d$ is generated.

\begin{equation}
\label{eq:vectores}
(s)\mapsto \left(x_{t-(d-1)\Delta t},x_{t-(d-2)\Delta t},\cdots,x_{t-\Delta t},x_{t}\right) \ ,
\end{equation}

Each vector turns out to be the ``history'' of the value $x_t$.
Clearly, the longer the length of the vectors $d$, the more
information about the history would the vectors have. b) The
permutations $\pi=(r_0, r_1, \cdots, r_{d-1})$ of $(0, 1, \cdots,
d-1)$ are called ``order of patterns'' of time $t$, defined by:

\begin{equation}
\label{eq:permuta}
x_{t-r_{d-1}\Delta t}\le x_{t-r_{d-2}\Delta t}\le\cdots\le x_{t-r_{1}\Delta t}\le x_{t-r_0\Delta t}.
\end{equation}
%
In order to obtain an unique result it is considered $r_i
<r_{i-1}$ if $x_{t-r_{i}\Delta t}=x_{t-r_{i-1}\Delta t}$.

In this way, all the $d!$ possible permutations $\pi$ of order
$d$, and the PDF $P=\{p(\pi)\}$ is defined as:

\begin{equation}
\label{eq:frequ}
p(\pi)~=~ \frac{\sharp \{s|s\leq M-Dd+1; (s) \quad \texttt{has type}~\pi\}}{M-d+1}.
\end{equation}
In the last expression the $\sharp$ symbol means ``number".

This procedure has the advantages of being {\it i)\/} simple, {\it
ii)\/} fast to calculate, {\it iii)\/} robust in presence of
noise, and {\it iv)\/} invariant to lineal monotonous
transformations.

It is applicable to weak stationarity processes (for
$k=d$, the probability that $x_t < x_{t+k}$ doesn't depend on the
particulary $t$ \cite{Pompe2002}).The causality property of the
PDF allows the quantifiers (based on this PDFs) to discriminate
between deterministic and stochastic systems \cite{Rosso2007B}.

The choice of the embedding dimension $d$ is crucial because it
determines the minimal length acceptable of the original temporal
series ($M \gg d!$) needed to obtain an adequate statistics.
According to this point Bandt and Pompe suggested $3\leq d \leq
7$. $d=6$ has been adopted in this work.

Based on our previous research \cite{DeMicco2009} we have employed
the statistical complexity $C$ and the entropy $H$ to define a plane where the stochasticiy of the chaotic system may be represented. A full discussion about the convenience of using these quantifiers is
out of the scope of this work. Nevertheless reliable bibliographic
sources do exist
\cite{Wackerbauer1994,Lopez1995,Rosso2007A,DeMicco2008,Rosso2009,Martin2006}.

The entropy $H[P]$ is the normalized version of the Entropy proposed by Shannon \cite{Shannon1949a}:
\begin{equation}\label{eq:sha}
H[P] = S[P] /S_{max},
\end{equation}
where $S[P]=-\sum _{j=1}^{M}~p_j~\ln( p_j )$\\ and $S_{max}$ is
the normalizing constant:
\begin{equation}
\label{eq:Smax} S_{max}= S[P_e] = \ln M,
\end{equation}
and $P_e=\{ 1/M, \cdots,1/M\}$ is the uniform distribution. The number of symbols $M$ is equal to $N$ for $H_{hist}$ and it is equal to $D!$ for $H_{BP}$.

The statistical complexity $C[P]$ is given by:
\begin{equation}
\label{eq:inten}
C[{P}]=Q_{J}[{P,P_e}]\cdot H[{P}] \ ,
\end{equation}
, and
$Q_{J}$ is named ``disequilibrium'' and it is the distance between $P$ and $P_e$ 
 in the probability space. The metric used in this paper is based on the Jensen-Shannon divergence
 \cite{Lamberti2004}:
\begin{equation}
\label{eq:disequi}
Q_{J}[{P,P_e}]= Q_0 \cdot \{S[\frac{P+P_e}{2}]-S[P]/2-S[P_e]/2 \} \ .
\end{equation}
The normalization constant $Q_0$ is:
\begin{equation}
\label{eq:q0j}
Q_0=-2 \left\{ \left( \frac{N+1}{N} \right) \ln(N+1) - 2 \ln(2N) + \ln N \right\}^{-1} .
\end{equation}

From the statistical point of view the disequilibrium $Q_J$ is an
intensive magnitude, and it is $0$ if and only if $P=P_e$. It has
been proved that the $C[P]$ quantifies the presence of nonlinear
correlations typical of chaotic systems
\cite{Martin2003,Lamberti2004}. The complexity $C[P]$ is
independent from the entropy $H[P]$, as far as different $P$'s share
the same entropy $H[P]$ but they have different complexity
$C[P]$.

Two representation planes are considered: $H_{BP}$ vs $H_{hist}$ \cite{DeMicco2008} and $H_{BP}$ vs $C_{BP}$ \cite{Rosso2007C}. In the first plane a higher value in any of the entropies, $H_{BP}$ and $H_{hist}$, implies an increase in the uniformity of the involved $PDF$. The point $(1,1)$ represents the ideal case with uniform histogram and uniform distribution of ordering patterns. In the second plane not the entire region $0<H_{BP}<1$, $0<C_{BP}<1$ is achievable. In fact for any $PDF$ the pairs $(H,C)$ of possible values fall between two extreme curves in the plane $H$-$C$ \cite{Anteneodo1996}. Fig. \ref{fig:planezone} shows two regions labeled as \textit{deterministic} and \textit{stochastic}. In fact transition from one region to the other are smooth and the division is a bit arbitrary. A more detailed discussion can be seen in \cite{Rosso2007C}. Ideal random systems having uniform Bandt \& Pompe $PDF$, are represented by the point $(1,0)$ \cite{Gonzalez2005} and a delta-like $PDF$ corresponds with the point $(0,0)$. 
